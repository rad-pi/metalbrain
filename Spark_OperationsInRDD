Very useful reference for all codes: https://spark.apache.org/docs/2.4.4/api/python/pyspark.html#pyspark.RDD

The map() operation is one of the most useful transformations in Spark, as it helps mould the data into the required form before performing any functions.
Moreover, it also helps provide a certain structure to the data stored in the RDDs.
Another such function is flatMap(). An important point that was covered in the previous segment is the  map() and flatMap() transformations.

Some of the important transformations and actions available in Spark are as follows:

map()
The map() transformation takes a function or a lambda expression as an argument, and this function is executed on each data item of the parent RDD.
There is a one-to-one mapping between the elements of the source RDD and those of the resultant RDD.
The return value of the function, when applied to each data item of the parent RDD, forms the data items of the resultant RDD.

flatMap()
The flatMap() transformation maps an element of the input RDD to one or more elements in the target RDD.
Unlike the map()transformation, the lambda expression will return a collection of values for a single element in the input RDD.

filter()
The filter() transformation takes a function or a lambda expression that returns a boolean value as the argument.
This boolean function is executed on each element of the source RDD.
Only those elements that return 'true' when the lambda expression is executed on them find a place in the resultant RDD.



Some of the common actions that are applied to a basic RDD are as follows:

collect()
This action collects all the data for a particular RDD distributed across partitions.
The collected data is returned as an array to the Driver program.

take()
This action takes an integer parameter 'n' and returns a list of the first 'n' elements of the RDD to the Driver program.
The take() action is more suitable than the collect() action as the former loads only a part of the dataset in the Driver program.

saveAsTextFile()
This action is useful for storing all the data items present in the RDD in persistent storage such as a local file system or HDFS.

count()
This action returns the total number of elements that exist in RDDs.

------------------------------------------------------------
Example #1:
# rdd1
rdd1 = sc.parallelize(range(6), 2)

# Transforming the RDDs by applying a map() function
rdd2 = rdd1.map(lambda x: x*2)

# Collecting all the elements in the driver program
rdd2.collect()

# Filtering the elements based on a condition
rdd3 = rdd2.filter(lambda x: x>4)

# Collecting all the elements in the driver program
rdd3.collect()

------------------------------------------------------------
Example #2:
# Defining the lines RDD
lines = sc.parallelize(["upGrad and IITM offer the course on Machine Learning and Cloud Computing.", "There is an entrance exam for the course.", "Good students are welcome to do this course."])

# Note: Remove the backslash in case of error


# Splitting the lines into words using flatMap()
words = lines.flatMap(lambda x: x.split(" "))

# Counting the elements in RDDs
lines.count()
words.count()

# Split using the map() transformation
wordlines = lines.map(lambda x: x.split(" "))

# Counting the elements in RDDs
wordlines.count()

# Collecting the RDDs in the driver program
words.collect()
wordlines.collect()

------------------------------------------------------------
join() and intersection()
RDDs are a distributed set of elements on which you can implement all operations such as union(), join() and intersection()
Actions can also help you obtain the max(), min() and mean() values from the numerical RDDs.

CODE
# Defining the lines RDD
lines = sc.parallelize(["upGrad and IITM offer the course on Machine Learning and Cloud Computing.", "There is an entrance exam for the course.", "Good students are welcome to do this course."])

# Defining the extralines RDD
extralines = sc.parallelize(["Shiva is coordinator of the course from upGrad.", "Prof. Janakiram is the coordinator for the course from IIT Madras."])

# Merging the elements of two RDDs using union()
totallines = lines.union(extralines)

# Taking an intersection of the two RDDs
nulllines = lines.intersection(extralines)

# Collecting the RDDs in the driver program
totallines.collect()
nulllines.collect()

------------------------------------------------------------
Max, Min and Avg
# RDD - rdd1
rdd1  = sc.parallelize(range(6), 2)
Output: 0,1,2,3,4,5

# maximum value
max = rdd1.max()
max
Output:5

# minimum value
min = rdd1.min()
min
Output:0

# mean value
avg = rdd1.mean()
avg
Output:2.5

------------------------------------------------------------
SUM Function
# RDD - rdd1
rdd1 = sc.parallelize(range(6), 2)

# Sum is commutative and associative function
def sum(x, y):
return(x+y)

# Calculating the sum using reduce()
rdd1.reduce(sum)
https://images.upgrad.com/2087fafd-ba86-453f-9a72-a63b434b0e36-11.%20Reduce%20-%20Sum.jpg

------------------------------------------------------------
AVERAGE Function
# Average is not commutative and associative function
# Run the sImilar function and check the output
def average(x, y):
      return(x+y)/2.0

# Remember if you don't provide a float value,  the output will be an integer
# Therefore, use 2.0 instead of 2

# Calculating the average using reduce()
rdd1.reduce(average)

# Correct average = 2.5
https://images.upgrad.com/1c71911b-0a71-4a98-9bef-1129c0459fd9-10.%20Reduce%20-%20Avg.jpg
